#!/usr/bin/env python3

r"""Script to generate lml files from the autho sign up google form.

You must pass the sheet Identifier you wish to process.

To access google you have to do some setup
``https://developers.google.com/sheets/api/quickstart/python``

You need  client secret from
''https://console.cloud.google.com/auth''
roughly you must create a client secret for OAUTH using this wizard
``https://console.developers.google.com/start/api?id=sheets.googleapis.com``
Accept the blurb and go to the APIs
click CANCEL on the next screen to get to the `create credentials`
hit create credentials choose OATH client
Configure a product - just put in a name like `Rubin Authors`
Create web application id
Give it a name hit ok on the next screen
now you can download the client id - call it client_secret.json
as expected below.
You have to do this once to allow the script to access your google stuff
from this machine.
"""

import argparse
import os
import os.path
import pickle
import re
from typing import Any

import yaml
from authordb import Address, Affiliation, AuthorDbAuthor, dump_authordb, load_authordb
from google.auth.transport.requests import Request
from google_auth_oauthlib.flow import InstalledAppFlow
from googleapiclient.discovery import build
from oauth2client.client import Credentials
from pydantic import BaseModel, Field
from pylatexenc.latexencode import unicode_to_latex

# If modifying these scopes, delete your previously saved credentials
# at ~/.credentials/sheets.googleapis.com-python-quickstart.json
SCOPES = ["https://www.googleapis.com/auth/spreadsheets.readonly"]
CLIENT_SECRET_FILE = "client_secret.json"
APPLICATION_NAME = "Process Authors Google Sheet"
script_name = os.path.basename(__file__)


# Load column index map from YAML if available, else use defaults
try:
    with open("column_map.yaml") as f:
        column_map = yaml.safe_load(f) or {}
except FileNotFoundError:
    column_map = {}
except Exception as e:
    print(f"Warning: could not load column_map.yaml ({e}), using defaults.")
    column_map = {}

print(f"Using colum_map:{column_map}")

# default matches the AUTHORDB update form
EMAIL = column_map.get("EMAIL", 1)
UPDATE = column_map.get("UPDATE", 3)
AUTHORID = column_map.get("AUTHORID", 4)
AUTHORIDALT = column_map.get("AUTHORIDALT", 6)
SURNAME = column_map.get("SURNAME", 5)
NAME = column_map.get("NAME", 6)
AFFIL = column_map.get("AFFIL", 7)
ORCID = column_map.get("ORCID", 8)


def get_credentials() -> Credentials:
    """Get valid user credentials from storage.

    If nothing has been stored, or if the stored credentials are invalid,
    the OAuth2 flow is completed to obtain the new credentials.

    Returns
    -------
    creds : `Credentials`
        Credentials, the obtained credential.
    """
    # The file token.pickle stores the user's access and refresh tokens, and is
    # created automatically when the authorization flow completes for the first
    # time.
    creds = None
    if os.path.exists("token.pickle"):
        with open("token.pickle", "rb") as token:
            creds = pickle.load(token)
    # If there are no (valid) credentials available, let the user log in.
    if not creds or not creds.valid:
        if creds and creds.expired and creds.refresh_token:
            creds.refresh(Request())
        else:
            flow = InstalledAppFlow.from_client_secrets_file(CLIENT_SECRET_FILE, SCOPES)
            creds = flow.run_local_server(port=0, access_type="offline", prompt="consent")
        # Save the credentials for the next run
        with open("token.pickle", "wb") as token:
            pickle.dump(creds, token)
    return creds


class AuthorYaml(BaseModel):
    """Model for the author dict temp file."""

    authors: dict[str, AuthorDbAuthor] = Field(description="Mapping of author IDs to author information")


class AffilYaml(BaseModel):
    """Model for the author dict temp file."""

    affiliations: dict[str, Affiliation] = Field(description="Mapping of affil IDs to affil information")


def get_initials(names: str, div: str = " ") -> str:
    """Get first letter of word (upto 5) to make an id"""
    all = names.split(div)
    initials = ""
    count = 0
    for n in all:
        count = count + 1
        if len(n) > 0 and n[0].isalpha():
            initials = f"{initials}{n[0]}"
        if count > 5:
            break
    return initials


def write_yaml(name: str, values: Any) -> None:
    """Write given dat to  the file name  YAML"""
    with open(name, "w") as file:
        file.write(f"# this file` was generated by {script_name} do not edit\n")
        yaml.dump(values, file)


def write_model(name: str, authors: dict[str, AuthorDbAuthor]) -> None:
    """Write given data to  the file name  YAML"""
    adb = AuthorYaml(authors=authors)
    with open(name, "w") as file:
        yaml.dump(adb.model_dump(), file)


def write_affil(name: str, affils: dict[str, Affiliation]) -> None:
    """Write given data to  the file name  YAML"""
    adb = AffilYaml(affiliations=affils)
    with open(name, "w") as file:
        yaml.dump(adb.model_dump(), file)


def load_model(filename: str) -> dict[str, AuthorDbAuthor]:
    """Read authors  data from the YAML file"""
    with open(filename) as file:
        yaml_data = yaml.safe_load(file)
        print("Parsing into AuthorYaml object...\n")
        adb = AuthorYaml.model_validate(yaml_data)
    return adb.authors


def handle_email(
    email: str,
    affils: dict[str, Affiliation],
    affilid: str,
    newaffils: dict[str, Affiliation],
) -> str:
    """Return local-part or local-part@affilid, updating new_affiliations
    when we discover a new email domain. We do not maintain a domains dict.
    """
    theEmail = email
    if "@" not in email:
        return theEmail

    mailid, domain = email.split("@", 1)

    # 1) If the given affil already has this domain, return just local-part.
    if affilid in affils:
        if affils[affilid].email == domain:
            return f"{mailid}"
        if not affils[affilid].email or affils[affilid].email == "null":
            # we have an institue with an empty email
            newaffils[affilid] = affils[affilid]
            newaffils[affilid].email = domain
            return f"{mailid}"

    # 2) If any *other* affiliation owns this domain, encode as @that_affilid.
    for aid, a in affils.items():
        if a.email == domain:
            return f"{mailid}@{aid}"

    # 3) New domain: attach it to the provided affilid  if staged,
    if affilid in newaffils:
        newaffils[affilid].email = domain
        return f"{mailid}"
    else:
        # Create a minimal placeholder affiliation carrying just the email.
        # Adjust required fields if your Affiliation model is stricter.
        daffil = domain.split(".")[0]
        newaffils[daffil] = Affiliation(
            institute=f"Email only for {domain}",
            address=None,
            email=domain,
        )
        # Now that this domain is associated to affilid, encode as @affilid
        return f"{mailid}@{daffil}"


def strip_utf(ins: str) -> str:
    """Want simple ids wiht now latex or unicode"""
    outs = unicode_to_latex(ins)
    outs = re.sub(r"\'", "", outs)
    outs = re.sub(r"[-'}{]", "", outs)
    outs = outs.replace("\\", "")
    outs = outs.replace("-", "")
    return outs


def lower_strip_utf(s: str) -> str:
    """Lower case, strip utf chars and other chars and space"""
    id = strip_utf(s)  # no utf
    id = id.lower().replace(" ", "")
    return id


def make_id(name: str, surname: str) -> str:
    """Make id form surnameinitials no space lowecase"""
    initials = get_initials(name)
    id = lower_strip_utf(f"{surname}{initials}")  # last name initial
    return id


def parse_affiliation(affil_str: str) -> Affiliation:
    """Parse an affiliation string into an Affiliation object."""
    parts = [p.strip() for p in affil_str.split(",")]
    if len(parts) < 2:
        raise ValueError("Affiliation string does not have enough parts")
    name = parts[0]
    address_str = unicode_to_latex(", ".join(parts[1:]))
    # Try to extract country (last part)
    country = parts[-1]
    # Try to extract postal code (look for 5-digit number)
    postal_code_match = re.search(r"\b\d{5}\b", address_str)
    postal_code = postal_code_match.group(0) if postal_code_match else ""
    # The rest is street and city
    street_and_city = address_str.replace(country, "").replace(postal_code, "").strip(", ")
    address = Address(
        example_expanded=unicode_to_latex(affil_str),
        street=street_and_city,
        city="",  # Optionally parse city if possible
        postcode=postal_code,
        country_code=country,
    )
    return Affiliation(institute=unicode_to_latex(name), address=address)


def process_affiliations(
    row: list[Any],
    idx: int,
    affils: dict[str, Affiliation],
    newaffils: dict[str, Affiliation],
    missing_affils: list[str],
) -> list[str]:
    """
    Process the AFFIL column from a row and update affiliations.

    Parameters
    ----------
    row : list
        A row of data from the Google Sheet.
    idx : int
        Row index (for logging).
    id : str
        Author ID (for logging).
    affils : dict[str, Affiliation]
        Known affiliations (global database).
    newaffils : dict[str, Affiliation]
        Newly discovered affiliations (staged).
    missing_affils : list[str]
        List to append missing affiliation IDs.

    Returns
    -------
    affilids : list[str]
        List of affiliation IDs found/created for this row.
    """
    affilids = []

    if len(row) > AFFIL and len(row[AFFIL]) > 0:
        affilidForm = str(row[AFFIL]).strip().split("/")

        if affilidForm[0] == "AURA":
            # Chilean quirk: AURA/Rubin gets normalized
            affilidForm[0] = "RubinObsC"

        for affilid in affilidForm:
            if affilid not in affils:
                if len(affilid) < 10:
                    print(f"Affiliation does not exist: {affilid} - skipping {id} at index {idx}")
                    missing_affils.append(affilid)
                else:  # assume it's a new affiliation string
                    affil = affilid
                    if "rubin" in affil.lower():
                        affilid = "RubinObsC"
                    else:
                        affilid = get_initials(affil)
                        try:
                            newaffils[affilid] = parse_affiliation(affil)
                            affils[affilid] = newaffils[affilid]
                        except Exception:
                            print(f"Affiliation parse failed: {affilid} - skipping {id} at index {idx}")
                            missing_affils.append(affilid)
            affilids.append(affilid)

        if affilids and affilids[0] not in affils:
            return []  # skip this row, primary affiliation missing

    return affilids


def genFilesADB(values: list, skip: int) -> None:
    """Assumes we are procsssing the AuthordDB update form.
    It will produce:
        new_authors.yaml - authors that need to be created/updated.
        new_affiliations.yaml - new affilliations inc new emails
        No authors.yaml

    values contains the selected cells from a google sheet with format
    with cells mapping set up in the start of the routine but contains
    Email Address
    Affirmation
    authorid (May be blank)
    surname (aka family name or last name)
    First names,
    affilid
    ORCID (optional)
    """
    if not values:
        print("No data found.")
    else:
        clash = []
        check = []
        bad = []
        toupdate = []
        notfound = []
        newauthors: dict[str, AuthorDbAuthor] = {}
        newaffils: dict[str, Affiliation] = {}
        authordb = load_authordb()
        authors = authordb.authors
        affils = authordb.affiliations
        missing_affils: list[str] = []  #  for missing affiliations

        for idx, row in enumerate(values):
            id = str(row[AUTHORID]).replace(" ", "")
            newid = len(id) == 0 or id.upper() == "NEW" or id.upper() == "NUEVO"
            if idx < skip:
                if id not in authors:
                    print(f"Skipping author {id} - but that author was not found in authordb")
                continue  # Skip this for update/new

            if newid:
                id = make_id(row[NAME], row[SURNAME])
                if id in authors and idx >= skip:
                    print(f"Perhaps check  clash - author {id} - {row[AUTHORID]}, @{idx} ")
                    clash.append(id)
                elif idx >= skip:
                    print(f"New author {id} - {row[NAME]}, {row[SURNAME]} ")
            else:
                print(f"Update author {id} at index {idx} ")
                if id not in authors:
                    print(f"      but  author {id} - NOT FOUND ")
                    notfound.append(id)
                toupdate.append(id)
            # we have an id or a new id now check it
            if len(row) >= SURNAME and not id.startswith(lower_strip_utf(row[SURNAME])):
                # this can be a foreign charecter
                badid = id
                id = make_id(row[NAME], row[SURNAME])
                if id != badid:  # really there is a problem
                    check.append(id)
                    print(f"Check  - author provided {badid}  at {idx}- assuming {id} - {row[SURNAME]}")
            elif id.lower() != id:
                bad.append(id)
                print(f"Check - author id {id} at index {idx}: id is not all lowercase")

            # update or new - get the other fields

            if len(row) > AFFIL and len(row[AFFIL]) > 0:
                affilids = process_affiliations(row, idx, affils, newaffils, missing_affils)
                if len(row) > ORCID:
                    orc = str(row[ORCID]).strip().replace("https://orcid.org/", "")
                    if len(orc) < 2:
                        orcid = None
                    else:
                        orcid = orc
                else:
                    orcid = None
                email: str = handle_email(row[EMAIL], affils, affilids[0], newaffils)
                if id in authors and not newid:
                    oauthor = authors[id]
                    if orcid is None:  # dont clobber
                        orcid = oauthor.orcid
                author: AuthorDbAuthor = AuthorDbAuthor(
                    given_name=unicode_to_latex(row[NAME].strip()),
                    family_name=unicode_to_latex(row[SURNAME].strip()),
                    orcid=orcid,
                    email=email,
                    affil=affilids,
                    altaffil=[],
                )
                newauthors[id] = author
        print(
            "\n"
            f" Clash: {', '.join(clash)} \n"
            f" Not FOUND: {', '.join(notfound)} \n"
            f" Missing Affils: {', '.join(missing_affils)} \n"
            f" Check the ids: {', '.join(check)} \n"
            f"{len(newauthors) - len(toupdate)} new  and {len(toupdate)} to update, author entries.\n"
            f"{len(newaffils)} new affiliations (inc. any email domains) \n"
            f"{len(clash)} author entries need to be checked (see above) \n"
            f"{len(notfound)} author updates where authorid not found (see above) \n"
            f" Saw: {idx + 1} rows - skip file contains the number for reprocessing \n"
        )
        with open("skip", "w") as f:
            f.write(f"{idx + 1}\n")

        write_model("new_authors.yaml", newauthors)
        write_affil("new_affiliations.yaml", newaffils)

    return


def genFiles(values: list, skip: int, builder: bool = False, adb: bool = False) -> None:
    """** Delete this when we are done with DP1 paper Generate Files.
    authors.yaml - with all authorids (if not adb)
    new_authors.yaml - authors that need to be created/updated.
    new_affiliations.yaml - with new affilliations inc new emails

    values contains the selected cells from a google sheet with format
    with cells mapping set up in the start of the routine but containitns
    Email Address
    Affirmation
    authorid (May be blank)
    surname (aka family name or last name)
    First names,
    affilid
    ORCID (optional)
    """
    if not values:
        print("No data found.")
    else:
        authorids = []
        clash = []
        check = []
        bad = []
        toupdate = []
        notfound = []
        newauthors: dict[str, AuthorDbAuthor] = {}
        newaffils: dict[str, Affiliation] = {}
        authordb = load_authordb()
        authors = authordb.authors
        affils = authordb.affiliations
        missing_affils: list[str] = []  #  for missing affiliations

        for idx, row in enumerate(values):
            id = str(row[AUTHORID]).replace(" ", "")
            newid = False
            if len(id) == 0:  # may be an update
                if adb:  # adb we know its new and there is no ALTID
                    newid = True
                else:
                    id = str(row[AUTHORIDALT]).strip()
                    newid = len(id) == 0 or id.upper() == "NEW" or id.upper() == "NUEVO"
                if newid:
                    id = make_id(row[NAME], row[SURNAME])
            if len(row) >= SURNAME and not id.startswith(lower_strip_utf(row[SURNAME])):
                # this can be a foreign charecter
                badid = id
                id = make_id(row[NAME], row[SURNAME])
                if id != badid:  # really there is a problem
                    check.append(id)
                    print(
                        f"Check  - author provided {badid}  at {idx}- assuming {id} - "
                        f"{row[SURNAME]}, {row[AUTHORIDALT]} "
                    )
            update = (adb and not newid) or "but" in row[UPDATE]
            if update and idx >= skip:
                print(f"Update author {id} at index {idx} ")
                if id not in authors:
                    print(f"      but  author {id} - NOT FOUND ")
                    notfound.append(id)
                else:
                    print(f"Perhaps check  clash - author {id} - {row[AUTHORID]}, @{idx} ")
                    clash.append(id)
                toupdate.append(id)
            if newid and idx >= skip:
                print(f"New author {id} - {row[NAME]}, {row[SURNAME]} ")
            # we have an id or a new id now
            # checks
            if len(row) >= SURNAME and not id.startswith(lower_strip_utf(row[SURNAME])):
                bad.append(id)
                print(
                    f"Check - author id {id} at index {idx}: does not start with "
                    f"surname '{row[SURNAME].strip().lower()}'"
                )
            elif id.lower() != id:
                bad.append(id)
                print(f"Check - author id {id} at index {idx}: id is not all lowercase")
            # we are collecting all the ids - skip is only to not make NEW ones
            if id in authorids:
                print(f"Duplicate  {id} at index {idx} ")
            else:
                if id not in bad:
                    authorids.append(id)

            if idx < skip:
                if id not in authors:
                    print(f"Skipping author {id} - but that author was not found in authordb")
                continue  # Skip this for update/new
            # next are we updating or creating?
            if len(row) > AFFIL and len(row[AFFIL]) > 0:
                affilids = process_affiliations(row, idx, affils, newaffils, missing_affils)
                if len(row) > ORCID:
                    orc = str(row[ORCID]).strip().replace("https://orcid.org/", "")
                    if len(orc) < 2:
                        orcid = None
                    else:
                        orcid = orc
                else:
                    orcid = None
                email: str = handle_email(row[EMAIL], affils, affilids[0], newaffils)
                if id in authors and not newid:
                    oauthor = authors[id]
                    if orcid is None:  # dont clobber
                        orcid = oauthor.orcid
                author: AuthorDbAuthor = AuthorDbAuthor(
                    given_name=unicode_to_latex(row[NAME].strip()),
                    family_name=unicode_to_latex(row[SURNAME].strip()),
                    orcid=orcid,
                    email=email,
                    affil=affilids,
                    altaffil=[],
                )
                newauthors[id] = author
        authorids = sorted(authorids)
        build = ""
        if builder and "RubinBuilderPaper" not in authorids:
            authorids.insert(0, "RubinBuilderPaper")
            build = "(including RubinBuilderPaper)"
        print(
            "\n"
            f" Clash: {', '.join(clash)} \n"
            f" Not FOUND: {', '.join(notfound)} \n"
            f" Missing Affils: {', '.join(missing_affils)} \n"
            f" Check the ids: {', '.join(check)} \n"
            f"got {len(authorids)} authors {build}, "
            f"{len(newauthors) - len(toupdate)} new  and {len(toupdate)} to update, author entries.\n"
            f" {len(newaffils)} new affiliations (inc. any email domains) \n"
            f" {len(clash)} author entries need to be checked (see above) \n"
            f" {len(notfound)} author updates where authorid not found (see above) \n"
            f" Saw: {idx + 1} rows - skip file contains the number for reprocessing \n"
        )
        with open("skip", "w") as f:
            f.write(f"{idx + 1}\n")

        if not adb:
            write_yaml("authors.yaml", authorids)
        write_model("new_authors.yaml", newauthors)
        write_affil("new_affiliations.yaml", newaffils)

    return


def get_sheet(sheet_id: str, range: str) -> dict[str, Any]:
    """Grab the google sheet and return data from sheet.

    Parameters
    ----------
    sheet_id : `str`
        GoogelSheet Id like
        ``1R1h41KVtN2gKXJAVzd4KLlcF-FnNhpt1G06YhzwuWiY``
    range : `str`
        List of ``TabName!An:Zn``  ranges
    """
    creds = get_credentials()
    service = build("sheets", "v4", credentials=creds)
    sheet = service.spreadsheets()
    result = sheet.values().get(spreadsheetId=sheet_id, range=range).execute()
    return result


def process_google(
    sheet_id: str, sheets: str, skip: int = 0, builder: bool = False, adb: bool = False
) -> None:
    """Grab the googlesheet and process data.
    will create new_authors and new_afilliations

    Parameters
    ----------
    sheet_id : str
        The Google Sheet ID to process.
    sheets : str
        One or more sheet ranges (e.g., 'Sheet1!A1:D').
    skip : int, optional
        Number of initial rows to skip when processing (default is 0).
        they will be adde to the auhots.yaml but not flagged as new/update
        assumign they were already added to authordb
    builder : bool, optional
        If True, add 'RubinBuilderPaper' as the first author ID in the output.
    adb: bool, optional
        If true we are processing the ADB update form - so e.g. no authors.yaml

    """
    print(f"Processing Google Sheet ID: {sheet_id}")
    print(f"Sheet ranges: {sheets}")
    for r in sheets:
        result = get_sheet(sheet_id, r)
        values: list[Any] = result.get("values", [])
        if skip > 0:
            print(f"Skipping the first {skip} lines of the sheet for new authors.")
        if adb:
            print(f"Doing ADB {adb}")
            genFilesADB(values, skip)
        else:  # this will go away after DP1
            genFiles(values, skip, builder=builder)


def process_signup(
    sheet_id: str,
    sheets: list[str],
    authorid_col: int = 4,
    skip: int = 0,
    builder: bool = False,
) -> None:
    """Simplified signup processor:
    - Reads only the AuthorID column (authorid_col) from the given sheets.
    - Produces authors.yaml (sorted, de-duplicated).
    - Optional: insert RubinBuilderPaper as the first entry when builder=True.
    """
    print(f"[signup] Processing Google Sheet ID: {sheet_id}")
    print(f"[signup] Sheet ranges: {sheets}")
    print(f"[signup] Using AuthorID column index: {authorid_col}")
    authordb = load_authordb()
    authors = authordb.authors

    authorids_set: set[str] = set()
    last_idx = -1

    for r in sheets:
        result = get_sheet(sheet_id, r)
        values: list[Any] = result.get("values", [])
        if not values:
            print(f"[signup] No data found for range {r}")
            continue

        if skip > 0:
            print(f"[signup] Skipping the first {skip} rows")

        for idx, row in enumerate(values):
            last_idx = idx
            if idx <= skip:
                continue
            raw = str(row[authorid_col]).strip()
            if not raw:
                print(f"Signup problem at idx {idx}")
                continue
            # normalize similar : remove spaces, lower-case
            aid = raw.replace(" ", "").lower()
            if aid in authors:
                authorids_set.add(aid)
            else:
                print(f"Invalid authorid {aid} - will not be added")

    # prepare ordered list
    authorids = sorted(authorids_set)

    if builder and "rubinbuilderpaper" not in authorids_set:
        # keep legacy case for output id
        authorids.insert(0, "RubinBuilderPaper")

    # brief summary
    print(
        "\n"
        f"[signup] collected {len(authorids)} author ids\n"
        f"[signup] last processed row index: {last_idx} - written to signup_skip\n"
    )

    # write skip file
    with open("signup_skip", "w") as f:
        f.write(f"{last_idx}\n")

    write_yaml("authors.yaml", authorids)


def merge_authors_with_update(adb: dict[str, AuthorDbAuthor], authors: dict[str, AuthorDbAuthor]) -> None:
    """
    Merge authors into adb.authors.
    - If author not in adb, add it.
    - If author exists and has ORCID in adb but not in new entry,
      copy ORCID to new entry and replace in adb.
    """
    for author_id, new_author in authors.items():
        if author_id in adb:
            existing_author = adb[author_id]
            if existing_author.orcid and not new_author.orcid:
                new_author.orcid = existing_author.orcid
        adb[author_id] = new_author


def merge_authors(author_file: str) -> None:
    """Take the given author yaml file and merge to authordd
    this file shold mathc the AuthorYaml class in authordb.py
    """
    print(f"Merging authors using file: {author_file}")
    authors = load_model(author_file)
    adb = load_authordb()
    print(f"Have {len(adb.authors)} authors")
    merge_authors_with_update(adb.authors, authors)
    print(f"After update have {len(adb.authors)} authors")
    dump_authordb(adb)


def merge_affiliations(affil_file: str) -> None:
    """Merge affiliations from the given YAML file into the authordb."""
    print(f"Merging affiliations using file: {affil_file}")
    with open(affil_file) as file:
        yaml_data = yaml.safe_load(file)
        adb = load_authordb()
        affil_yaml = AffilYaml.model_validate(yaml_data)
        print(f"Have {len(adb.affiliations)} affiliations")
        adb.affiliations.update(affil_yaml.affiliations)
        print(f"After update have {len(adb.affiliations)} affiliations")
        dump_authordb(adb)


if __name__ == "__main__":
    description = __doc__ or "Process Google Sheets and merge authors."
    formatter = argparse.RawDescriptionHelpFormatter

    parser = argparse.ArgumentParser(description=description, formatter_class=formatter)

    # Required --process-google with 2+ args: id + sheets
    parser.add_argument(
        "-p",
        "--process-google",
        nargs="+",
        metavar=("ID", "SHEET"),
        help="Process Google Sheet: provide the ID and one or more sheet ranges (e.g. Model!A1:D)",
    )

    # Optional --merge-authors with one file
    parser.add_argument(
        "-m",
        "--merge-authors",
        metavar="AUTHOR_FILE",
        help="Path to YAML file to use for merging authors",
    )

    parser.add_argument(
        "-a",
        "--merge-affiliations",
        metavar="AFFIL_FILE",
        help="Path to YAML file to use for merging affiliations",
    )

    parser.add_argument(
        "-s",
        "--skip",
        type=int,
        default=0,
        metavar="N",
        help="Skip the first N lines of the Google Sheet data",
    )
    parser.add_argument(
        "-b",
        "--builder",
        action="store_true",
        help="Add RubinBuilderPaper as the first author id in the generated file",
    )

    parser.add_argument(  # can not have -a used for affil
        "--adb",
        action="store_true",
        help="Process Google Sheet assuming is the new author form no "
        "authors.yaml (only new_authors, new_affiliations)"
        "expects -p sheet range",
    )

    parser.add_argument(
        "-S",
        "--signup",
        nargs="?",  # optional argument
        const=4,  # default if used without a value
        type=int,
        metavar="AUTHORID_COL",
        help="Signup mode: process the Google Sheet write authors.yaml "
        "The integer argument specifies the column number of the AuthorID.",
    )

    args = parser.parse_args()

    did_something = False

    if args.process_google:
        sheet_id = args.process_google[0]
        sheet_ranges = args.process_google[1:]
        if args.builder and args.adb:
            print("Builder makes no sense with AuthorDB update - ignored please remove the flag.")
        if args.signup is not None:
            process_signup(
                sheet_id, sheet_ranges, skip=args.skip, builder=args.builder, authorid_col=args.signup
            )
        else:
            process_google(sheet_id, sheet_ranges, skip=args.skip, builder=args.builder, adb=args.adb)
        did_something = True

    if args.merge_authors:
        merge_authors(args.merge_authors)
        did_something = True

    if args.merge_affiliations:
        merge_affiliations(args.merge_affiliations)
        did_something = True

    if not did_something:
        parser.print_help()
